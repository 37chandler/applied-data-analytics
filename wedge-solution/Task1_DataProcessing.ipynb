{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook handles the tasks of Phase 1, getting the data out of the zip files and uploaded to GBQ. Data is in a folder of 53 zip files on my machine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile \n",
    "import io\n",
    "import csv\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "from wedge_functions import * # bring in our functions\n",
    "\n",
    "# If this is true, we delete the clean files after we've uplaoded them. \n",
    "clean_up_clean_files = True\n",
    "\n",
    "clean_out_gbq = True # change to true to empty the dataset, false requires a starting point below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do all our GBQ set-up so that piece is ready."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"C:\\\\users\\\\jchan\\\\dropbox\\\\teaching\\\\\"\n",
    "service_file = 'UMT-MSBA-7b4265df0ca4.json' # this is your authentication information  \n",
    "gbq_proj_id = 'umt-msba'  # change this to your project_id\n",
    "gbq_dataset_id = 'wedge_transactions' # and change this to your data set ID\n",
    "\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this to decide where to start back up. \n",
    "if clean_out_gbq :\n",
    "    clean_out_gbq_tables(client,gbq_proj_id,gbq_dataset_id)    \n",
    "else :\n",
    "    next_table_to_fill = \"transArchive_201401_201403\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some constants and get our list of zip files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_file_location = \"C:/users/jchan/dropbox/teaching/corporatepartners/wedge/data/holder/\"\n",
    "zip_location = \"C:/users/jchan/dropbox/teaching/corporatepartners/wedge/data/WedgeZipOfZips/\"\n",
    "\n",
    "zip_files = sorted(os.listdir(zip_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We didn't clean out the GBQ data set, so we're\n",
    "# going to start in the middle of the data. This is not\n",
    "# robust to reordering of files\n",
    "have_hit_restart = clean_out_gbq \n",
    "# If clean out gbq is false, set our flag to false\n",
    "# that tells us if we're ready to get back to work. \n",
    "\n",
    "for zf_idx, this_zip_file in enumerate(zip_files) :\n",
    "    if \".zip\" in this_zip_file :\n",
    "        print(\"Working on \" + this_zip_file)\n",
    "        zf = zipfile.ZipFile(\"\".join([zip_location,this_zip_file]))\n",
    "        compressed_files = zf.filelist\n",
    "        \n",
    "        # Make sure we only have one file in the zip.\n",
    "        assert(len(compressed_files) == 1)\n",
    "        \n",
    "        file_name = compressed_files[0]\n",
    "\n",
    "        input_file = zf.open(file_name,'r')\n",
    "        input_file = io.TextIOWrapper(input_file,encoding=\"utf-8\")\n",
    "\n",
    "        input_file_name = file_name.filename\n",
    "        clean_file_name = input_file_name.replace(\".csv\",\"_clean.csv\")\n",
    "        table_name = input_file_name.replace(\".csv\",\"\")\n",
    "\n",
    "        # Now we check to see if we're supposed to process this table.\n",
    "        if not have_hit_restart :\n",
    "            if table_name == next_table_to_fill :\n",
    "                have_hit_restart = True\n",
    "\n",
    "                # delete this table so we can start from scratch\n",
    "                full_table_name = \".\".join([gbq_proj_id,\n",
    "                                              gbq_dataset_id,\n",
    "                                              table_name])\n",
    "                \n",
    "                if tbl_exists(client,full_table_name) :\n",
    "                    client.delete_table(full_table_name, \n",
    "                                        not_found_ok=True)\n",
    "\n",
    "                # and get rid of the csv\n",
    "                if os.path.isfile(clean_file_location + clean_file_name) :\n",
    "                    print(\"Deleting {} from clean file location.\".format(clean_file_name))\n",
    "                    os.unlink(clean_file_location + clean_file_name)\n",
    "                \n",
    "            else :\n",
    "                # if we haven't hit the restart, just jump to the next \n",
    "                # \"this_zip_file\" in the list\n",
    "                continue\n",
    "        \n",
    "        # Let's figure out the delimiter for this file\n",
    "        dialect = csv.Sniffer().sniff(sample=input_file.readline(),\n",
    "                                  delimiters=[\",\",\";\",\"\\t\"])\n",
    "\n",
    "        # csv.Sniffer moves you into a file. you have to go \n",
    "        # back to the beginning to avoid missing any rows\n",
    "        input_file.seek(0)\n",
    "\n",
    "        # Let's learn about headers and quotes \n",
    "        line = input_file.readline()\n",
    "\n",
    "        has_headers = \"datetime\" in line\n",
    "\n",
    "        # Let's test to see if we have lots of quotes in the\n",
    "        # file. If so, then there are quotes around each field, \n",
    "        # probably.\n",
    "        #has_quotes = line.count('\"') > 0\n",
    "        # Note, we'll still have to do line checking on the quote stuff.\n",
    "\n",
    "        # and back to the start\n",
    "        input_file.seek(0)\n",
    "\n",
    "        # If we have a header row, we'll skip it. We don't \n",
    "        # need this for GBQ. \n",
    "        if has_headers :\n",
    "            next(input_file)\n",
    "\n",
    "        with open(clean_file_location + clean_file_name,'w') as outfile :\n",
    "            for idx, line in enumerate(input_file) :\n",
    "\n",
    "                # Let's just get rid of the inconsistently used double quotes\n",
    "                line = line.replace('\"','')\n",
    "\n",
    "                line = line.strip().split(dialect.delimiter)\n",
    "                if len(line) > 50 :\n",
    "                    # If len > 50, we have split the description into \n",
    "                    # more than one field, probably because it's something like\n",
    "                    # \"Fruit, Granola, Yogurt\". This goofy line glues\n",
    "                    # those together with \";\" and puts the line back together\n",
    "                    line = (line[:5] + \n",
    "                            [\";\".join(line[5:(len(line)-44)])] + \n",
    "                            line[(len(line)-44):])\n",
    "\n",
    "                assert(len(line)==50)\n",
    "\n",
    "                oline = \",\".join([str(item) for item in line]) + \"\\n\"\n",
    "\n",
    "                # Let's get rid of NULLs in this string. \n",
    "                oline = oline.replace(\"NULL\",\"\")\n",
    "                oline = oline.replace(r\"\\\\N\",\"\")\n",
    "                oline = oline.replace(r\"\\N\",\"\")\n",
    "                # Note, order matters for those last two. r\"\\\\N\".replace(r\"\\N\",\"\") == r\"\\\"\n",
    "                # the \"r\" means the strings are raw, so \"\\n\" doesn't become a return, for instance\n",
    "\n",
    "                outfile.write(oline)\n",
    "\n",
    "            if idx % 10000 == 0 :\n",
    "                print(\"On line {} in {}.\".format(idx,input_file_name))\n",
    "\n",
    "                    \n",
    "        # The clean file is written out. Now we need to upload it to GBQ. \n",
    "        # Going to use some functions to make this a bit easier.\n",
    "        job_config = get_upload_job_config()\n",
    "                \n",
    "        with open(clean_file_location + clean_file_name, \"rb\") as source_file:\n",
    "            job = client.load_table_from_file(\n",
    "                source_file,\n",
    "                \".\".join([gbq_proj_id,gbq_dataset_id,table_name]),\n",
    "                location=\"US\",  \n",
    "                job_config=job_config,\n",
    "            )  \n",
    "                \n",
    "        job.result() \n",
    "        \n",
    "        print(\"Loaded {} rows into {}:{}.\".format(job.output_rows, gbq_dataset_id, table_name))\n",
    "\n",
    "        if clean_up_clean_files :\n",
    "            print(\"Deleting {} from clean file location.\".format(clean_file_name))\n",
    "            os.unlink(clean_file_location + clean_file_name)\n",
    "        \n",
    "        zf.close()\n",
    "        \n",
    "#        if table_name == \"transArchive_201106\" :\n",
    "#            break\n",
    "        \n",
    "        #if zf_idx > 2 :\n",
    "        #      break\n",
    "              \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
