{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping Candidates' Sites\n",
    "\n",
    "In this notebook we'll scrape candidates' websites and store the information for later analysis. We'll store the data locally in a database, which we'll build via Python to practice that skill. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import requests\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "from bs4.element import Comment\n",
    "\n",
    "import random \n",
    "# this package is good for randomization, which we may use \n",
    "# use when we're pulling a fraction of the pages. \n",
    "\n",
    "import datetime # this is good for working with dates and times. It's a bit confusing though. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build a DB for this. Right now I'm assuming we'll have five fields in our only table:\n",
    "\n",
    "* `dt`: the date and time when we pulled the page.\n",
    "* `base_url`: the main URL we're pulling from. E.g., www.joebiden.com, for Biden's site. \n",
    "* `url`: the specific URL we're pulling from. E.g., https://joebiden.com/joes-story/.\n",
    "* `text`: the text of the `url`. \n",
    "* `pulled`: A boolean that is TRUE if we've tried to pull the text from the URL. This is useful for keeping track. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db = sqlite3.connect(\"candidate_websites.db\") # feel free to change this to something you like. \n",
    "cur = db.cursor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create the table in the DB. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur.execute('''DROP TABLE IF EXISTS site_text''')\n",
    "cur.execute('''CREATE TABLE site_text (\n",
    "    dt DATETIME, \n",
    "    base_url TEXT, \n",
    "    url TEXT,\n",
    "    text TEXT,\n",
    "    pulled BOOLEAN)''')\n",
    "db.commit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll build off the previous notebook (`Intro to Scraping.ipynb`) to scrape candidates' sites. Let's begin by reading the list of websites.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = []\n",
    "with open(\"candidates_websites.txt\",'r') as infile :\n",
    "    for line in infile :\n",
    "        sites.append(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tag_visible(element):\n",
    "    if element.parent.name in ['style', 'script', 'head', 'title', 'meta', '[document]']:\n",
    "        return False\n",
    "    if isinstance(element, Comment):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://joebiden.com/\n",
      "https://berniesanders.com/\n",
      "https://elizabethwarren.com\n",
      "https://kamalaharris.org/\n",
      "https://peteforamerica.com/\n",
      "https://betoorourke.com/\n",
      "https://corybooker.com/\n",
      "https://amyklobuchar.com\n",
      "https://www.yang2020.com/\n",
      "https://www.julianforthefuture.com/\n",
      "https://www.tulsi2020.com/\n",
      "https://www.tomsteyer.com/\n"
     ]
    }
   ],
   "source": [
    "for this_site in sites :\n",
    "    \n",
    "    print(this_site)\n",
    "\n",
    "    # First get links on homepage\n",
    "    links = []\n",
    "    #this_site = sites[2]\n",
    "\n",
    "    r = requests.get(this_site)\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    for link in soup.find_all('a'):\n",
    "        links.append(link.get('href'))\n",
    "\n",
    "    links = [link for link in links if link]\n",
    "\n",
    "    # Now loop through links and get pages\n",
    "    for sub_link in sorted(set(links)) :\n",
    "        if sub_link and 'http' not in sub_link :\n",
    "            good_sub_link = this_site + sub_link\n",
    "        else :\n",
    "            good_sub_link = sub_link\n",
    "            \n",
    "        if 'mailto' in good_sub_link :\n",
    "            continue\n",
    "\n",
    "        r = requests.get(good_sub_link)\n",
    "\n",
    "        if r.status_code == 200 :\n",
    "            soup = BeautifulSoup(r.text, 'html.parser')\n",
    "            texts = soup.findAll(text=True)\n",
    "            visible_texts = filter(tag_visible, texts) \n",
    "            page_text = \" \".join(t.strip() for t in visible_texts)\n",
    "\n",
    "        new_row = [datetime.datetime.now(),\n",
    "                   this_site,\n",
    "                   good_sub_link,\n",
    "                   page_text, \n",
    "                   1]\n",
    "\n",
    "        cur.execute('''INSERT INTO site_text (dt,base_url,url,text,pulled) \n",
    "               VALUES (?,?,?,?,?)''',new_row)\n",
    "\n",
    "    db.commit() # don't forget this!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = \"adkkdd; dkdkdkd; 1334; akdkdkd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = line.split(\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'adkkdd, dkdkdkd, 1334, akdkdkd'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\",\".join(line)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
