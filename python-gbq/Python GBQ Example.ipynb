{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Google Big Query Example\n",
    "\n",
    "This notebook explores your ability to connect a Python notebook to a Google Big Query instance. *NOTE*: Before you try to run this notebook, you need to _complete_ steps 1 and 2 from this [page](https://cloud.google.com/bigquery/docs/reference/libraries#client-libraries-install-python). For step 2, \"Setting up Authentication\", I recommend following the console directions. Pay attention to where you save your JSON authentication file, since you'll need to tell Python how to find it. \n",
    "\n",
    "There's a second part of step 2, where you set an environment variable at the command line to alert to your credentials. I was never able to make that work on my machine, so I'm using a direct path in my code below.That second step of that requires a bit of command line work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"C:\\\\users\\\\jchan\\\\dropbox\\\\teaching\\\\\"\n",
    "service_file = 'UMT-MSBA-7b4265df0ca4.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'umt-msba' # change this to your poroject. \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our `client` variable holds a connection to the project. This is client is similar to a file handle--it allows you to \"talk\" to the project. Let's begin by writing a query and running it. \n",
    "\n",
    "---\n",
    "\n",
    "### Querying Data\n",
    "\n",
    "In this next section we'll query some data from our GBQ instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example query. This uses a trick that Python concatenates adjacent string literals for you.\n",
    "# Note, you'll need to update the project_id and dataset_id in the query below (and throughout)\n",
    "query = (\n",
    "    \"SELECT card_no, sum(total) as TotalSales \"\n",
    "    \"FROM `umt-msba.wedge_example.transactions_201307_small` \"\n",
    "    \"WHERE total > 0 AND trans_type = 'I' \"\n",
    "    \"GROUP BY card_no \"\n",
    "    \"ORDER BY TotalSales DESC \"\n",
    ")\n",
    "\n",
    "# And we execute queries with `client.query`\n",
    "query_job = client.query(\n",
    "    query,\n",
    "    location=\"US\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we've called `client.query` and assigned the results, we can iterate over the result set. In this case we'll print the first 6 rows in a nice format. Note that `row`, the name we give to the items produced when we iterate over  `query_job` is a tuple of length two. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, row in enumerate(query_job) :\n",
    "    card, sales = row\n",
    "    print(\"Card {:.0f} spent {:,.2f} dollars.\".format(card,sales))\n",
    "    \n",
    "    if idx == 5 :\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Uploading Data\n",
    "\n",
    "To add data to the dataset, we'll need to create a table and put data in it. We'll do a simple example here to illustrate the concept. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uploading Manually\n",
    "\n",
    "This is the easist way, but it only works for small files. Go to the console, and click on our project, `umt-msba`, and then on our dataset, `wedge_example`. You'll see a blue \"+\" sign (in the new UI). Click on that and it will open a \"create table\" prompt. \n",
    "\n",
    "Once that's open, here are the options to choose as you go down: \n",
    "\n",
    "1. Create from \"Upload\"\n",
    "1. Select the file `department_lookup.csv` that's in the same folder as this notebook.\n",
    "1. Give your table a unique name. I'm using `chandler_test1` here.  \n",
    "1. Select \"Auto detect\" under \"Schema\"\n",
    "\n",
    "You don't need to worry about the other fields and checkboxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have data in the file that you can see via the console. Let's test it here.\n",
    "query = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM `umt-msba.wedge_example.chandler_test1` \" # change to *your*` project ID, data set ID, and table name! \n",
    ")\n",
    "\n",
    "results = client.query(query)\n",
    "print(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Table Creation, Programmatic Upload\n",
    "\n",
    "Now we'll do something similar. We'll create the table manually but upload the data programmatically.  \n",
    "\n",
    "So go through similar steps to create the table via the console. Click on our project, `umt-msba`, and then on our dataset, `wedge_example`, and then click on the create table plus sign again. This opens the \"create table\" prompt. \n",
    "\n",
    "Once that's open, here are the options to choose as you go down: \n",
    "\n",
    "1. Create from \"Empty Table\"\n",
    "1. Give your table a unique name. I'm using `chandler_test2`. \n",
    "1. Add two fields. One called \"department\" that will be an integer and one called \"dept_name\" that will be a string.\n",
    "\n",
    "You don't need to worry about the other fields and checkboxes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by querying the data and seeing that it's empty.\n",
    "\n",
    "query = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM `umt-msba.wedge_example.chandler_test2` \" # change to *your*` project ID, data set ID, and table name!\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and now run the query\n",
    "results = client.query(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(results)) # Should be an empty list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's put some data in the table. This is a bit messy *and* only works for small data sets, so I'll show you how this is done, but we won't probably use this technique in the future. This code is taken directly from the GBQ [docs on uploading data](https://cloud.google.com/bigquery/docs/loading-data-local#limitations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ref = client.dataset(\"wedge_example\") # set up references to the dataset and table.\n",
    "table_ref = dataset_ref.table(\"chandler_test2\") # use your name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to establish a job that will run this import for us. A clunky step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_config = bigquery.LoadJobConfig() \n",
    "job_config.source_format = bigquery.SourceFormat.CSV\n",
    "job_config.skip_leading_rows = 1\n",
    "job_config.autodetect = True\n",
    "input_file = \"department_lookup.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(input_file, \"rb\") as source_file:\n",
    "    job = client.load_table_from_file(source_file, table_ref, job_config=job_config)\n",
    "    \n",
    "job.result()  # Waits for table load to complete.\n",
    "\n",
    "print(\"Loaded {} rows into {}:{}.\".format(job.output_rows, dataset_ref, table_ref))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's query this table and see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You should have data in the file that you can see via the console. Let's test it here.\n",
    "query = (\n",
    "    \"SELECT * \"\n",
    "    \"FROM `umt-msba.wedge_example.chandler_test2` \" # change to *your*` project ID, data set ID, and table name! \n",
    ")\n",
    "\n",
    "results = client.query(query)\n",
    "print(list(results))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "When you're working on the Wedge project, the table schema is *much* more complicated. If you've reached this point in the notebook and have some extra time, see if you can download the table schema from one of our Wedge tables.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
